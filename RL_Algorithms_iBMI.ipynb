{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Lightweight Reinforcement Algorithms for Autonomous, Scalable, Neuromorphic intra-cortical Brain Machine Interfaces**\n",
        "\n",
        "This work discusses the application of five popular online RL algorithms in the context of intention decoders for iBMI systems. RL based iBMI implementations are promising as they tackle the issue of frequent re-training and calibrations; also they adopt an online learning strategy, which is resonant with ‘how the brain performs.’ We compare the discrete-state decoding performances of online learning RL algorithms like **Banditron**, and **Banditron-RP (proposed)** against state-of-the-art batch-training based RL algorithms like **Attention Gate Reinforcement Learning (AGREL)**, **Hebbian Reinforcement Learning (HRL)**, and **Deep Q-Learning**. \n",
        "\n",
        "\\\\\n",
        "**Dataset used:** The datasets are recorded in-house at A*star. A total of four datasets, two from NHP A (experiments 1 and 3) and two from NHP B (experiments 2 and 4) respectively. Here we have evaluated on a small publicly available [dataset](https://osf.io/dce96/).\n",
        "\n",
        "\\\\\n",
        "**Performance metric:** Classification accuracy (decoding accuracy)\n",
        "\n",
        "**Reference:** Ghosh A., and Shaikh S. et al., Lightweight Reinforcement Learning Decoders for Autonomous, Scalable, Neuromorphic intra-cortical Brain Machine Interface; submitted Neuromorphic Computing & Interface, 2023.\n",
        "\n",
        "\\\\\n",
        "**Version:** v1.0\n",
        "\n",
        "**License:** Please see the accompanying file named \"LICENSE\"\n",
        "\n",
        "**Author:** Aayushman Ghosh, University of Illinois Urbana Champaign, May 2023. (<aghosh14@illinois.edu>)"
      ],
      "metadata": {
        "id": "6OzLTjw3Mha_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGD5LxATkCCG"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries and modules.\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from scipy.io import loadmat\n",
        "import pandas as pd\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.activations import relu\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model, Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "import scipy.special as sp\n",
        "\n",
        "scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_XMI6SXscKG"
      },
      "outputs": [],
      "source": [
        "# Defining the Global Variables --> Directory, error and sparsity error.\n",
        "dir = r'/content/drive/MyDrive/Datasets/BMI_datasets/classification_dataset/monkey_2_set_2' # Parent directory that contain the processed classification files.\n",
        "\n",
        "error = 0             # Defining the error in Feedback\n",
        "sparsity_rate = 0     # Sparsity in the Feedback signals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th7p0RkaLoUc",
        "outputId": "605dff9b-337a-4c6a-82d9-bc53ce34a19f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mounting the google drive to load the dataset as needed.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-GV0EuusgEr"
      },
      "outputs": [],
      "source": [
        "# Defining the sigmoid function.\n",
        "def sigmoid(x):\n",
        "  z = 1/(1+sp.expit(-x))\n",
        "  return z"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the ReLU function.\n",
        "def ReLU(x):\n",
        "  z = np.maximum(0,x)\n",
        "  return z"
      ],
      "metadata": {
        "id": "25DuPOo7e2ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DsRVIvd5GSQ"
      },
      "outputs": [],
      "source": [
        "# Defining the softmax function.\n",
        "def softmax(x):\n",
        "  x = x - np.max(x)\n",
        "  z = np.exp(x)/np.sum(np.exp(x))\n",
        "  return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYTetWKXEdIx"
      },
      "outputs": [],
      "source": [
        "# Defining the Banditron Function (Single Layered Network)\n",
        "'''\n",
        "Here, **kwargs is used to denote the arbitary input functions. The error and sparse_rate\n",
        "corresponds to the error and the sparsity introduced to the feedback signal (refer to the \n",
        "paper to understand the physical significance). k denotes the number of classes and is \n",
        "fixed at 4 for our experiments. X denotes the spike count data (observation -- input dataset) \n",
        "and y denotes the true labels associated with each observation (X) \n",
        "'''\n",
        "def banditron(X, y, error, sparse_rate, k=4, **kwargs):\n",
        "    T = X.shape[0] \n",
        "    d = X.shape[1]\n",
        "    np.random.seed(100) \n",
        "    W = np.zeros((k, d)) # Initializing the Weight Matrix. \n",
        "    error_count = np.zeros(T) \n",
        "    pred = [] # The predicted labels will be stored here.\n",
        "    \n",
        "    # The exploration exploitation constant and eta are given as optional arguments.\n",
        "    if \"gammas\" not in kwargs:\n",
        "        gammas = [kwargs[\"gamma\"] for i in range(T)]\n",
        "    else:\n",
        "        gammas = kwargs[\"gammas\"]\n",
        "    \n",
        "    if \"eta\" in kwargs:\n",
        "        eta = kwargs[\"eta\"]\n",
        "    \n",
        "    # Evaluative framework (refer to the paper to understand the mathematics)\n",
        "    for t in range(T):\n",
        "        gamma = gammas[t]\n",
        "        y_hat = np.argmax(np.dot(W, X[t]))\n",
        "        p = [gamma/k for i in range(k)]\n",
        "        p[y_hat] = p[y_hat] + 1 - gamma\n",
        "        y_tilde = np.random.choice(range(k), p=p)\n",
        "        pred.append(y_tilde)\n",
        "        sparsify = np.random.choice([True,False],p=[sparse_rate,1-sparse_rate])\n",
        "        if not sparsify:\n",
        "          if y_tilde != y[t]:\n",
        "            choice = np.random.choice(range(2),p=[error,1-error])\n",
        "            if choice == 1:\n",
        "              W[y_hat] = W[y_hat] - X[t]   \n",
        "            else:\n",
        "              W[y_hat] = W[y_hat] - X[t]\n",
        "              W[y_tilde] = W[y_tilde] + X[t] / p[y_tilde]         \n",
        "          else:\n",
        "            choice = np.random.choice(range(2),p=[error,1-error])\n",
        "            if choice == 1:\n",
        "              W[y_hat] = W[y_hat] - X[t]\n",
        "              W[y_tilde] = W[y_tilde] + X[t] / p[y_tilde]\n",
        "            else:\n",
        "              W[y_hat] = W[y_hat] - X[t]       \n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1hbC-acGIkj"
      },
      "outputs": [],
      "source": [
        "# Defining the Banditron-RP Function (Three Layered Network)\n",
        "'''\n",
        "Here, **kwargs is used to denote the arbitary input functions. The error and sparse_rate\n",
        "corresponds to the error and the sparsity introduced to the feedback signal (refer to the \n",
        "paper to understand the physical significance). k denotes the number of classes and is \n",
        "fixed at 4 for our experiments. X denotes the spike count data (observation -- input dataset) \n",
        "and y denotes the true labels associated with each observation (X) \n",
        "'''\n",
        "def banditronRP(X, y, k, error, sparse_rate, **kwargs):\n",
        "    d = X.shape[1]\n",
        "    Wrand = np.random.uniform(size=(k,d)) # The random Weight matrix generated from a normal distribution.\n",
        "    f = sigmoid(np.dot(Wrand,X.T)) # The non-linear projection vector input to the hidden layer.\n",
        "    pred = banditron(f.T, y, error, sparsity_rate, **kwargs) # f(t) = Sigmoid(Wrand.x(t)) is given as an input to the Banditron.\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FadSKVPDtIu"
      },
      "outputs": [],
      "source": [
        "# Defining the HRL function (Three Layered Network)\n",
        "# Initializing the weight matrices\n",
        "'''\n",
        "In HRL the weight matrices are not initialized as zero matrix instead random floats\n",
        "are extracted from Gaussian Distribution (mean = 0, var = 1). Here inp_shp denotes the\n",
        "number of rows, and out_shp the number of columns for the weight matrix.\n",
        "'''\n",
        "def initialize(inp_shp,out_shp):\n",
        "  W = np.random.randn(out_shp,inp_shp)\n",
        "  return W\n",
        "\n",
        "'''\n",
        "The error and sparse_rate corresponds to the error and the sparsity introduced to the \n",
        "feedback signal (refer to the paper to understand the physical significance). muH and muO\n",
        "denotes the learning rates corresponding to the weight updation policy. num_nodes is a matrix referring to \n",
        "the number of hidden nodes and output nodes. X denotes the spike count data (observation -- input dataset) \n",
        "and y denotes the true labels associated with each observation (X) \n",
        "'''  \n",
        "def HRL(X, y, muH, muO, num_nodes, error, sparse_rate):\n",
        "    T = X.shape[0]\n",
        "    W = [0]*(len(num_nodes)-1)\n",
        "    pred = []\n",
        "    num_nodes[0] = num_nodes[0]+1\n",
        "    \n",
        "    # Initializing the weight matrices.\n",
        "    for i in range(1,len(num_nodes)):\n",
        "      W[i-1] = initialize(num_nodes[i-1],num_nodes[i])\n",
        "    \n",
        "    # Computing the output for the hidden layer and output layer.\n",
        "    for t in range(T):\n",
        "      x = np.insert(X[t],0,1)\n",
        "      out = [x.reshape(-1,1)]*(len(num_nodes))     \n",
        "      for i in range(1,len(num_nodes)):\n",
        "        out[i] = np.tanh(np.dot(W[i-1],out[i-1]))\n",
        "      \n",
        "      # Evaluative framework (refer to the paper to understand the mathematics)\n",
        "      out[-1] = np.tanh(np.dot(W[-1],np.sign(out[-2])))\n",
        "      yhat = np.argmax(out[-1])\n",
        "      sparsify = np.random.choice([True,False],p=[sparse_rate,1-sparse_rate])\n",
        "      if not sparsify:\n",
        "        if yhat == y[t]:\n",
        "          f = np.random.choice([-1,1],p=[error,1-error])\n",
        "        else:\n",
        "          f = np.random.choice([1,-1],p=[error,1-error])\n",
        "        dW = [0]*(len(num_nodes)-1)\n",
        "  \n",
        "        for i in range(1,len(num_nodes)):\n",
        "          dW[i-1] = muH*f*(np.dot((np.sign(out[i])-out[i]),out[i-1].T)) + muH*(1-f)*(np.dot((1-np.sign(out[i])-out[i]),out[i-1].T))\n",
        "          W[i-1] = W[i-1] + dW[i-1]\n",
        "\n",
        "        dW[-1] = muO*f*(np.dot((np.sign(out[-1])-out[-1]),out[-2].T)) + muO*(1-f)*(np.dot((1-np.sign(out[-1])-out[-1]),out[-2].T))\n",
        "        W[-1] = W[-1] + dW[-1]\n",
        "\n",
        "      pred.append(yhat)\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NOgcJCyw0tLz"
      },
      "outputs": [],
      "source": [
        "# Defining the AGREL function (Three Layered Network)\n",
        "# Initializing the weight matrices\n",
        "'''\n",
        "In AGREL the weight matrices are not initialized as zero matrix instead random floats\n",
        "are extracted from Gaussian Distribution (mean = 0, var = 1). Here inp_shp denotes the\n",
        "number of rows, and out_shp the number of columns for the weight matrix.\n",
        "'''\n",
        "def initialize(inp_shp,out_shp):\n",
        "  W = np.random.uniform(low=-1,high=1,size=(out_shp,inp_shp))\n",
        "  return W\n",
        "\n",
        "'''\n",
        "The error and sparse_rate corresponds to the error and the sparsity introduced to the \n",
        "feedback signal (refer to the paper to understand the physical significance). alpha, and beta\n",
        "denotes the learning rates corresponding to the weight updation policy. num_nodes is a matrix referring to \n",
        "the number of hidden nodes and output nodes. gamma denotes the exploration-exploitation trade-off. \n",
        "X denotes the spike count data (observation -- input dataset) and y denotes the true labels associated with each observation (X) \n",
        "''' \n",
        "def AGREL(X, y, gamma, alpha, beta, num_nodes, error, sparse_rate):\n",
        "    T = X.shape[0]\n",
        "    pred = []\n",
        "    \n",
        "    # Initializing the weight matrices.\n",
        "    W_H = initialize(num_nodes[0]+1,num_nodes[1])\n",
        "    W_O = initialize(num_nodes[1],num_nodes[2])\n",
        "\n",
        "    for t in range(T):\n",
        "      x = np.insert(X[t],0,1).reshape(-1,1) \n",
        "      y_H = sigmoid(np.dot(W_H,x))\n",
        "      Z = np.dot(W_O,y_H)\n",
        "      y_O = softmax(Z)\n",
        "      yhat = np.argmax(y_O)\n",
        "      \n",
        "      # Computing the output for the hidden layer and output layer.\n",
        "      outs = np.zeros(Z.shape)\n",
        "      outs[yhat] = 1\n",
        "      explore = np.random.uniform()<gamma \n",
        "      \n",
        "      # Evaluative framework (refer to the paper to understand the mathematics)\n",
        "      if explore:\n",
        "        y_tilde = np.random.randint(low=0,high=num_nodes[-1])\n",
        "      else:\n",
        "        y_tilde = yhat\n",
        "      sparsify = np.random.choice([True,False],p=[sparse_rate,1-sparse_rate])\n",
        "      if not sparsify:\n",
        "        if y_tilde == y[t]:\n",
        "          delta = np.random.choice([-1 ,1 - outs[y_tilde]],p=[error,1-error])\n",
        "        else:\n",
        "          delta = np.random.choice([-1 ,1 - outs[y_tilde]],p=[1-error,error])\n",
        "        '''\n",
        "        In AGREL, δ therefore influences plasticity through\n",
        "        an expansive function f (δ), which also helps to fasten\n",
        "        the learning process. f (δ) is defined such as, it takes\n",
        "        large values if δ is close to 1, that is, when actions are\n",
        "        rewarded unexpectedly.\n",
        "        '''\n",
        "        if delta >= 0:\n",
        "          f = delta/(1-delta+1e-4) \n",
        "        else:\n",
        "          f = delta\n",
        "        # Weight updation policy of AGREL\n",
        "        dW_O = beta*f*y_H.T\n",
        "        dW_H = alpha*f*np.dot((y_H*(1-y_H)*W_O[y_tilde,:].reshape(-1,1)),x.T)\n",
        "\n",
        "        W_O[y_tilde,:] = W_O[y_tilde,:] + dW_O\n",
        "        W_H = W_H + dW_H\n",
        "        \n",
        "      pred.append(yhat)\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6aiDDWb9B5S"
      },
      "outputs": [],
      "source": [
        "# Defining the DQN function (Four Layered Network)\n",
        "# Defining the DQN model\n",
        "'''\n",
        "Here, the Deep Q Learning model is computed as a four layer network, where the number of\n",
        "nodes in the input layer changes with the experiment, and is given by inp_shp. This \n",
        "following function sequentially builds the model, where the output layer has 4 classes,\n",
        "and the two hidden layers has 128 neurons.\n",
        "'''\n",
        "def get_DQN_model(inp_shp, lr=0.01):\n",
        "  np.random.seed(101)\n",
        "  model = Sequential()\n",
        "  model.add(Dense(128, activation='relu', input_shape=(inp_shp,))) # hidden layer1 neurons = 128\n",
        "  model.add(Dense(128, activation='relu')) # hidden layer2 neurons = 128\n",
        "  model.add(Dense(4, activation='linear')) # Output layer neurons = 4\n",
        "  model.compile(loss='mse',optimizer='adam') # A mse loss function is used with an adam optimizer.\n",
        "  return model\n",
        "\n",
        "'''\n",
        "The error and sparse_rate corresponds to the error and the sparsity introduced to the \n",
        "feedback signal (refer to the paper to understand the physical significance). epsilon and gamma \n",
        "denotes the exploration constant and discount factor. X denotes the spike count data \n",
        "(observation -- input dataset) and y denotes the true labels associated with each observation (X). \n",
        "'''\n",
        "def DQN(X,Y,epsilon,gamma,error,sparse_rate):\n",
        "\n",
        "  inp_shp = X.shape[1] # Number of electrodes --> corresponding to the no. of neurons in the first layer.\n",
        "  model = get_DQN_model(inp_shp)\n",
        "  T = X.shape[0]\n",
        "  X_norm = scaler.fit_transform(X)\n",
        "  pred = []\n",
        "  \n",
        "  # Evaluative framework (refer to the paper to understand the mathematics)\n",
        "  for t in range(T):\n",
        "    x = X_norm[t,:].reshape(1,X.shape[1]).astype(np.float32)\n",
        "    y = Y[t,:].reshape(1,Y.shape[1])\n",
        "    Q = model.predict(x)\n",
        "\n",
        "    yhat = np.argmax(Q)\n",
        "    explore = np.random.uniform() < epsilon\n",
        "    if explore:\n",
        "      ytilde = np.random.randint(low=0,high=4)\n",
        "    else:\n",
        "      ytilde = yhat\n",
        "    sparsify = np.random.choice([True,False],p=[sparse_rate,1-sparse_rate])\n",
        "    if not sparsify:\n",
        "      if ytilde == np.argmax(y):\n",
        "        r = np.random.choice([-1 ,1],p=[error,1-error])\n",
        "      else:\n",
        "        r = np.random.choice([1 ,-1],p=[error,1-error])\n",
        "\n",
        "      Target = r + gamma*np.amax(Q)\n",
        "      Target_vec = Q\n",
        "      Target_vec[0,ytilde] = Target\n",
        "      Target_vec = np.array(Target_vec).reshape(1,4)\n",
        "\n",
        "      model.fit(x,Target_vec,batch_size=1, epochs=1,verbose=0)\n",
        "\n",
        "    pred.append(ytilde)\n",
        "  return np.array(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8B2lPsFmP7Vm"
      },
      "outputs": [],
      "source": [
        "# Defining the LGBM based Q-Learning Function\n",
        "# Building the LGBM Model\n",
        "'''\n",
        "Here, instead of using a deep neural network to build the DQN framework, we have \n",
        "used the LightGBM framework with a Q-Learning policy. We call this model the QLGBM Network.\n",
        "'''\n",
        "def get_QLGBM_model():\n",
        "  model = MultiOutputRegressor(LGBMRegressor(n_jobs=-1)) \n",
        "  return model\n",
        "\n",
        "'''\n",
        "The error and sparse_rate corresponds to the error and the sparsity introduced to the \n",
        "feedback signal (refer to the paper to understand the physical significance). epsilon and gamma \n",
        "denotes the exploration constant and discount factor. X denotes the spike count data \n",
        "(observation -- input dataset) and y denotes the true labels associated with each observation (X). \n",
        "'''\n",
        "def QLGBM(X,Y,epsilon,gamma,error,sparse_rate):\n",
        "  model = get_QLGBM_model()\n",
        "  T = X.shape[0]\n",
        "  X_norm = scaler.fit_transform(X)\n",
        "  pred = []\n",
        "  isFit = False\n",
        "  \n",
        "  # Evaluative framework (refer to the paper to understand the mathematics)\n",
        "  for t in range(T-1):\n",
        "    x = X_norm[t:t+2,:].astype(np.float32)\n",
        "    y = Y[t:t+2,:]\n",
        "    if isFit:\n",
        "      Q = model.predict(x)\n",
        "    else:\n",
        "      np.random.seed(101)\n",
        "      Q = np.random.uniform(low=-1,high=1,size=(2,4))\n",
        "    yhat = np.argmax(Q[0,:])\n",
        "    explore = np.random.uniform() < epsilon\n",
        "    if explore:\n",
        "      ytilde = np.random.randint(low=0,high=4)\n",
        "    else:\n",
        "      ytilde = yhat\n",
        "    sparsify = np.random.choice([True,False],p=[sparse_rate,1-sparse_rate])\n",
        "    if not sparsify:\n",
        "      if ytilde == np.argmax(y):\n",
        "        r = np.random.choice([-1 ,1],p=[error,1-error])\n",
        "      else:\n",
        "        r = np.random.choice([1 ,-1],p=[error,1-error])\n",
        "      Target = r + gamma*np.amax(Q)\n",
        "      Target_vec = Q\n",
        "      Target_vec[0,ytilde] = Target\n",
        "      Target_vec = np.array(Target_vec).reshape(2,4)\n",
        "      model.fit(x,Target_vec)\n",
        "      isFit = True\n",
        "    pred.append(ytilde)\n",
        "  return np.array(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iyY7yojFq3pP"
      },
      "outputs": [],
      "source": [
        "# Evaluating the Banditron Algorithm\n",
        "directory = dir\n",
        "files = os.listdir(directory)\n",
        "acc = []\n",
        "error_acc = []\n",
        "interval = []\n",
        "'''\n",
        "Defining the Exploration constant \n",
        "'''\n",
        "gamma = 0.0001 \n",
        "\n",
        "for file in files:\n",
        "    data = loadmat(os.path.join(directory,file))\n",
        "    feature_mat = data[\"feature_mat\"]\n",
        "    X = feature_mat[:,:-1]\n",
        "    y = feature_mat[:,-1]//90\n",
        "    pred = banditron(X, y, error, sparsity_rate, gamma=gamma)\n",
        "    results = np.vstack([y*90,np.array(pred)*90]).T\n",
        "    results_df = pd.DataFrame(results,columns=[\"True\",\"Pred\"])\n",
        "    error_intervals = 1/(np.sum(results_df.loc[:,\"True\"] == results_df.loc[:,\"Pred\"])/results_df.shape[0]*100)\n",
        "    error_acc.append(error_intervals)\n",
        "    interval.append(2.58*np.sqrt(error_intervals*(1-error_intervals))/results_df.shape[0])\n",
        "    acc.append((np.sum(results_df.loc[:,\"True\"] == results_df.loc[:,\"Pred\"])/results_df.shape[0])*100)\n",
        "\n",
        "acc_Banditron = acc\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(files)+1),acc,'b-o')\n",
        "plt.grid()\n",
        "plt.ylim((20,120))\n",
        "plt.ylabel('Accuracy (in %)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "for x,y in zip(range(1,len(files)+1),acc):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(-10,-20)) # distance from text to points (x,y)\n",
        "                 \n",
        "plt.title(directory.split('/')[-1]+' (Banditron) ')\n",
        "plt.show()\n",
        "print(np.mean(acc_Banditron))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYqyVIHJ7p1S"
      },
      "outputs": [],
      "source": [
        "# Evaluating the Banditron-RP Algorithm\n",
        "directory = dir\n",
        "files = os.listdir(directory)\n",
        "acc = []\n",
        "\n",
        "for file in files:\n",
        "    data = loadmat(os.path.join(directory,file))\n",
        "    feature_mat = data[\"feature_mat\"]\n",
        "    X = feature_mat[:,:-1]\n",
        "    y = feature_mat[:,-1]//90\n",
        "    pred = banditronRP(X, y, 128, error, sparsity_rate, gamma=gamma)\n",
        "    results = np.vstack([y*90,np.array(pred)*90]).T\n",
        "    results_df = pd.DataFrame(results,columns=[\"True\",\"Pred\"])\n",
        "    acc.append((np.sum(results_df.loc[:,\"True\"] == results_df.loc[:,\"Pred\"])/results_df.shape[0])*100)\n",
        "\n",
        "acc_BanditronRP = acc\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(files)+1),acc,'b-o')\n",
        "plt.grid()\n",
        "plt.ylim((20,120))\n",
        "plt.ylabel('Accuracy (in %)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "for x,y in zip(range(1,len(files)+1),acc):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(-10,-20)) # distance from text to points (x,y)\n",
        "                 \n",
        "plt.title(directory.split('/')[-1]+' (BanditronRP) ')\n",
        "plt.show()\n",
        "print(np.mean(acc_BanditronRP))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfcTjoNEdwhx"
      },
      "outputs": [],
      "source": [
        "# Evaluating the HRL Algorithm\n",
        "directory = dir\n",
        "files = os.listdir(directory)\n",
        "acc = []\n",
        "muH = 0.01         # Hidden Layer Learning rate\n",
        "muO = 0.01        # Output Layer Learning rate\n",
        "num_nodes=[100,4]   # Hidden Layer Node sizes\n",
        "\n",
        "for file in files:\n",
        "    data = loadmat(os.path.join(directory,file))\n",
        "    feature_mat = data[\"feature_mat\"]\n",
        "    X = feature_mat[:,:-1]\n",
        "    y = feature_mat[:,-1]//90\n",
        "    num_nodes=[75,4]\n",
        "    num_nodes.insert(0,X.shape[1])\n",
        "    pred = HRL(X, y, muH, muO, num_nodes, error, sparsity_rate)\n",
        "    results = np.vstack([y*90,np.array(pred)*90]).T\n",
        "    results_df = pd.DataFrame(results,columns=[\"True\",\"Pred\"])\n",
        "    acc.append((np.sum(results_df.loc[:,\"True\"] == results_df.loc[:,\"Pred\"])/results_df.shape[0])*100)\n",
        "\n",
        "acc_HRL = acc\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(files)+1),acc,'b-o')\n",
        "plt.grid()\n",
        "plt.ylim((20,120))\n",
        "plt.ylabel('Accuracy (in %)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "for x,y in zip(range(1,len(files)+1),acc):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(-10,-20)) # distance from text to points (x,y)\n",
        "                 \n",
        "plt.title(directory.split('/')[-1]+' (HRL) ')\n",
        "plt.show()\n",
        "print(np.mean(acc_HRL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fETA2PgOA11l"
      },
      "outputs": [],
      "source": [
        "# Evaluating the AGREL Algorithm\n",
        "directory = dir\n",
        "files = os.listdir(directory)\n",
        "acc = []\n",
        "alpha = 0.1\n",
        "beta = 0.1\n",
        "gamma = 0.02\n",
        "num_nodes=[1000,4]   # Hidden Layer Node sizes\n",
        "\n",
        "for file in files:\n",
        "    data = loadmat(os.path.join(directory,file))\n",
        "    feature_mat = data[\"feature_mat\"]\n",
        "    X = feature_mat[:,:-1]\n",
        "    y = feature_mat[:,-1]//90\n",
        "    num_nodes=[75,4]   # Hidden Layer Node sizes\n",
        "    num_nodes.insert(0,X.shape[1])\n",
        "    pred = AGREL(X, y, gamma, alpha, beta, num_nodes, error, sparsity_rate)\n",
        "    results = np.vstack([y*90,np.array(pred)*90]).T\n",
        "    results_df = pd.DataFrame(results,columns=[\"True\",\"Pred\"])\n",
        "    acc.append((np.sum(results_df.loc[:,\"True\"] == results_df.loc[:,\"Pred\"])/results_df.shape[0])*100)\n",
        "\n",
        "acc_AGREL = acc\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(files)+1),acc,'b-o')\n",
        "plt.grid()\n",
        "plt.ylim((20,120))\n",
        "plt.ylabel('Accuracy (in %)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "for x,y in zip(range(1,len(files)+1),acc):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(-10,-20)) # distance from text to points (x,y)\n",
        "                 \n",
        "plt.title(directory.split('/')[-1]+' (AGREL) ')\n",
        "plt.show()\n",
        "print(np.mean(acc_AGREL))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DP0xERzH9gSr"
      },
      "outputs": [],
      "source": [
        "# Evaluating the Q-Learning Algorithm\n",
        "directory = dir\n",
        "files = os.listdir(directory)\n",
        "acc = []\n",
        "exec_times_DQN = []\n",
        "epsilon = 0.01    # Exploration rate\n",
        "gamma = 0.1       # Discount Factor\n",
        "\n",
        "for file in files:\n",
        "    data = loadmat(os.path.join(directory,file))\n",
        "    feature_mat = data[\"feature_mat\"]\n",
        "    X = feature_mat[:,:-1]\n",
        "    y = feature_mat[:,-1]//90\n",
        "    target = to_categorical(y,4)\n",
        "    start = time.time()\n",
        "    pred = DQN(X,target,epsilon,gamma,error,sparsity_rate)\n",
        "    end = time.time()\n",
        "    exec_times_DQN.append(round(((end-start)*1e3/X.shape[0]),2))\n",
        "    results = np.vstack([y*90,np.array(pred)*90]).T\n",
        "    results_df = pd.DataFrame(results,columns=[\"True\",\"Pred\"])\n",
        "    acc.append((np.sum(results_df.loc[:,\"True\"] == results_df.loc[:,\"Pred\"])/results_df.shape[0])*100)\n",
        "\n",
        "acc_DQN = acc\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(files)+1),acc,'b-o')\n",
        "plt.grid()\n",
        "plt.ylim((20,120))\n",
        "plt.ylabel('Accuracy (in %)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "for x,y in zip(range(1,len(files)+1),acc):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(-10,-20)) # distance from text to points (x,y)\n",
        "                 \n",
        "plt.title(directory.split('/')[-1]+' (Deep Q-Learning) ')\n",
        "plt.show()\n",
        "print(np.mean(acc_DQN))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2VDE1-sQ9S0"
      },
      "outputs": [],
      "source": [
        "# Evaluating the QLGBM Algorithm\n",
        "directory = dir\n",
        "files = os.listdir(directory)\n",
        "acc = []\n",
        "exec_times_QLGBM = []\n",
        "\n",
        "for file in files:\n",
        "    data = loadmat(os.path.join(directory,file))\n",
        "    feature_mat = data[\"feature_mat\"]\n",
        "    X = feature_mat[:,:-1]\n",
        "    y = feature_mat[:,-1]//90\n",
        "    target = to_categorical(y,4)\n",
        "    start = time.time()\n",
        "    pred = QLGBM(X,target,epsilon,gamma,error,sparsity_rate)\n",
        "    end = time.time()\n",
        "    exec_times_QLGBM.append(round(((end-start)*1e3/(X.shape[0]-1)),2))\n",
        "    results = np.vstack([y[:-1]*90,np.array(pred)*90]).T\n",
        "    results_df = pd.DataFrame(results,columns=[\"True\",\"Pred\"])\n",
        "    acc.append((np.sum(results_df.loc[:,\"True\"] == results_df.loc[:,\"Pred\"])/results_df.shape[0])*100)\n",
        "\n",
        "acc_QLGBM = acc\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(range(1,len(files)+1),acc,'b-o')\n",
        "plt.grid()\n",
        "plt.ylim((20,120))\n",
        "plt.ylabel('Accuracy (in %)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "for x,y in zip(range(1,len(files)+1),acc):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(-10,-20)) # distance from text to points (x,y)\n",
        "                 \n",
        "plt.title(directory.split('/')[-1]+' (LightGBM based Q-Learning) ')\n",
        "plt.show()\n",
        "print(np.mean(acc_QLGBM))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pd_Auew_MTd"
      },
      "outputs": [],
      "source": [
        "# Plotting the Decoding accuracy of all the Algorithms\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.plot(range(1,len(files)+1),acc_Banditron,'b--o')\n",
        "plt.plot(range(1,len(files)+1),acc_BanditronRP,'r--o')\n",
        "plt.plot(range(1,len(files)+1),acc_HRL,'k--o')\n",
        "plt.plot(range(1,len(files)+1),acc_AGREL,'g--o')\n",
        "plt.plot(range(1,len(files)+1),acc_DQN,'y--o')\n",
        "plt.plot(range(1,len(files)+1),acc_QLGBM,'--o')\n",
        "\n",
        "plt.legend(['Banditron','BanditronRP','HRL','AGREL','Deep Q-Learning','LightGBM based Q-Learning'])\n",
        "\n",
        "plt.grid()\n",
        "plt.ylim((20,120))\n",
        "plt.ylabel('Accuracy (in %)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "                 \n",
        "plt.title(directory.split('/')[-1]+' Performance plots ')\n",
        "#plt.savefig(directory.split('/')[-1]+' Performance plots.jpg')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C5VX1kPDKNwi"
      },
      "outputs": [],
      "source": [
        "acc_df = pd.DataFrame([acc_Banditron,acc_BanditronRP,acc_HRL,acc_AGREL,acc_DQN,acc_QLGBM]).T\n",
        "acc_df.rename(dict(enumerate(['Banditron','BanditronRP','HRL','AGREL','Deep Q-Learning','QLGBM'])),axis=1,inplace=True)\n",
        "acc_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_Xb2Ti3eNRU"
      },
      "outputs": [],
      "source": [
        "# For taking out the accuracy data\n",
        "acc_df.to_csv(directory.split('/')[-1]+' Accuracies_0.75_sparse_feedback.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z95zBwhNneR"
      },
      "outputs": [],
      "source": [
        "# Comparing the execution time of DQN and QLGBM\n",
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "plt.plot(range(1,len(files)+1),exec_times_QLGBM,'-o')\n",
        "for x,y in zip(range(1,len(files)+1),exec_times_QLGBM):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(0,10)) # distance from text to points (x,y)\n",
        "  \n",
        "plt.plot(range(1,len(files)+1),exec_times_DQN,'-o')\n",
        "for x,y in zip(range(1,len(files)+1),exec_times_DQN):\n",
        "    label = \"{:.2f}\".format(y)\n",
        "    plt.annotate(label, # this is the text\n",
        "                 (x,y), # these are the coordinates to position the label\n",
        "                 textcoords=\"offset points\", # how to position the text\n",
        "                 xytext=(-10,-20)) # distance from text to points (x,y)\n",
        "  \n",
        "plt.ylim([0,100])\n",
        "plt.grid()\n",
        "plt.ylabel('Average Time per iteration (in milli-second)')\n",
        "plt.xlabel('Sessions')\n",
        "plt.xticks(range(1,len(files)+1), labels=['session_'+str(i) for i in range(1,len(files)+1)])\n",
        "plt.title('Execution times')\n",
        "plt.legend(['LightGBM based Q-Learning','Deep Q-Learning'])\n",
        "plt.savefig(directory.split('/')[-1] + 'Execution times.jpg')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}